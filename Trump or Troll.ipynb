{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump or Troll\n",
    "## Final Project: Shivam Jindal (sj2546) and Mayank Vanjani (mv1506)\n",
    "\n",
    "Classify tweets sent by President Trump versus Russian trolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(\"TrumpTwitterAll.xlsx\")\n",
    "df2 = pd.read_csv(\"IRAhandle_tweets_1.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trump_tweets_df is a dataframe containing all of the trump tweets from our dataset  \n",
    "troll_tweets_df is a dataframe containing all of the troll tweets from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30078, 5)\n",
      "(243891, 21)\n",
      "Index(['Date', 'Time', 'Tweet', 'Client', 'Client Simplified'], dtype='object')\n",
      "Index(['external_author_id', 'author', 'content', 'region', 'language',\n",
      "       'publish_date', 'harvested_date', 'following', 'followers', 'updates',\n",
      "       'post_type', 'account_type', 'retweet', 'account_category',\n",
      "       'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
      "       'tco1_step1', 'tco2_step1', 'tco3_step1'],\n",
      "      dtype='object')\n",
      "(30078,)\n"
     ]
    }
   ],
   "source": [
    "#df.head(5)\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "print(df.columns)\n",
    "print(df2.columns)\n",
    "\n",
    "trump_tweets_df = df[\"Tweet\"]\n",
    "troll_tweets_df = df2[\"content\"][:30078]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "trump_tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"We have a sitting Democrat US Senator on tria...\n",
       "1    Marshawn Lynch arrives to game in anti-Trump s...\n",
       "2    Daughter of fallen Navy Sailor delivers powerf...\n",
       "3    JUST IN: President Trump dedicates Presidents ...\n",
       "4    19,000 RESPECTING our National Anthem! #StandF...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "troll_tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( \n",
    "                max_df=0.5, # max doc freq (as a fraction) of any word to include in the vocabulary\n",
    "                min_df=2,   # min doc freq (as doc counts) of any word to include in the vocabulary\n",
    "                max_features=10000,           # max number of words in the vocabulary\n",
    "                stop_words='english',         # remove English stopwords\n",
    "                use_idf=True )        # use IDF scores\n",
    "all_my_tweeties = trump_tweets_df.append(troll_tweets_df)\n",
    "all_my_tweeties.head(5)\n",
    "\n",
    "X = vectorizer.fit_transform(all_my_tweeties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000    Thank you American Legion Post 610- for hosting @Mike_Pence & I for a roundtable with labor leaders. #LaborDay #MAGA https://t.co/r0cwJlV38L\n",
      "1000                                Buona domenica con la #buonalettura geopolitica della settimana  https://t.co/X17mpXcPfq https://t.co/YaZwFl0boc\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 2000\n",
    "\n",
    "print(all_my_tweeties[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor                0.428762 \n",
      "mike_pence           0.412798 \n",
      "hosting              0.394354 \n",
      "maga                 0.355841 \n",
      "leaders              0.342455 \n",
      "post                 0.329780 \n",
      "american             0.284062 \n",
      "thank                0.217162 \n",
      "https                0.098706 \n",
      "giulio               0.000000 \n"
     ]
    }
   ],
   "source": [
    "doc_ind = 1000  # Index of an example document\n",
    "xi = X[doc_ind,:].todense()\n",
    "term_ind = xi.argsort()[:, ::-1]\n",
    "xi_sort = xi[0,term_ind]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(10):\n",
    "    term = terms[term_ind[0,i]]\n",
    "    tfidf = xi[0,term_ind[0,i]]\n",
    "    print('{0:20s} {1:f} '.format(term, tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, init='k-means++', n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: https http trump great thank just people blacklivesmatter new enlist на like obama america don make di good time donald\n",
      "Cluster 1: realdonaldtrump thanks president great trump run donald thank mr love vote 2016 trump2016 need country true america best good make\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(2):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half = np.zeros(30078)\n",
    "second_half = first_half+1\n",
    "\n",
    "labels = np.append(first_half, second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40696152 0.98182005]\n",
      " [0.59303848 0.01817995]]\n"
     ]
    }
   ],
   "source": [
    "labelkm = km.labels_\n",
    "from sklearn.metrics import confusion_matrix\n",
    "C = confusion_matrix(labels,labelkm)\n",
    "\n",
    "Csum = np.sum(C, axis=0)\n",
    "Cnorm = C / Csum[None,:]\n",
    "print(Cnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
